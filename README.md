# âš½ Research Task 08 â€“ Bias Detection in LLM Narratives (FIFA Dataset)

**Author:** Shikha Singh  
**Course:** SU OPT Research â€“ Task 08  
**Advisor:** Jon Strome ([jrstrome@syr.edu](mailto:jrstrome@syr.edu))  
**Date:** November 2025  

---

## ğŸ§  Overview

This project investigates whether **Large Language Models (LLMs)** such as ChatGPT, Claude, and Gemini show **biases when interpreting identical data** framed differently.  
It builds on earlier OPT research tasks (Tasks 4â€“7) and applies controlled testing on an **anonymized FIFA player dataset**.

The experiment tests whether the modelâ€™s interpretation changes when:
- The **question framing** (positive vs. negative) changes,
- The **player demographics** are mentioned,
- Or a **prior belief** is stated in the prompt.

---

## ğŸ¯ Objectives

1. **Detect framing, demographic, and confirmation biases** in LLM outputs.  
2. **Quantify narrative differences** using structured prompts and sentiment patterns.  
3. **Validate statements** made by LLMs against real numeric ground truth from the FIFA dataset.  
4. **Document reproducible experiments** that meet Syracuse OPT research standards.

---

## ğŸ§© Methodology

### 1. Dataset Preparation
- Raw data: `Dataset/fifa_eda_stats.csv`  
- Anonymized with `sanitize_players.py` â†’ output: `data/fifa_anon.csv`  
  - Replaces real names with â€œPlayer Aâ€, â€œPlayer Bâ€, etc.  
  - Ensures no personally identifiable information is used.

### 2. Ground Truth Creation
- Script: `ground_truth.py`  
- Produces two reference files:
  - `analysis/ground_truth_full.csv` â€“ all numeric stats per player  
  - `analysis/ground_truth_metrics.csv` â€“ top players by key metrics (Finishing, Passing, GK, etc.)

### 3. Prompt Generation
- Script: `experiment_design.py`  
- Creates 5 structured prompt variations for each hypothesis:
  - **neutral**, **positive**, **negative**, **demographic**, **confirmation**  
- Output: `prompts/prompt_variations.csv`

### 4. Experiment Execution
- Each prompt is tested on 2â€“3 LLMs (ChatGPT, Claude, Gemini).  
- Raw text responses are stored under:
  - `results/raw/model_condition_runX.txt`  
- `run_experiment.py` merges all responses into a single CSV log for analysis.

### 5. Validation
- `validate_claims.py` cross-checks model statements against `ground_truth_metrics.csv`.  
- Flags contradictions, fabrications, or inconsistencies.  
- Output: `analysis/claims_validation.csv`

---

## ğŸ§ª Hypotheses Summary

| ID | Bias Type | Description | Example Prompt |
|----|------------|--------------|----------------|
| H0 | Neutral | Baseline question with stats only | â€œWhich player should get extra training?â€ |
| H1 | Framing | Positive vs. negative tone | â€œMost potential for improvementâ€ vs. â€œMost underperformingâ€ |
| H2 | Demographic | Mentions age/seniority | â€œSenior vs. Junior playersâ€¦â€ |
| H3 | Confirmation | Includes analystâ€™s opinion | â€œAnalysts believe Player A is best â€” do you agree?â€ |

---

## ğŸ“‚ Directory Structure

```plaintext
Task_08_Bias_Detection/
â”‚
â”œâ”€â”€ Dataset/
â”‚   â””â”€â”€ fifa_eda_stats.csv
â”œâ”€â”€ data/
â”‚   â””â”€â”€ fifa_anon.csv
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ ground_truth_full.csv
â”‚   â”œâ”€â”€ ground_truth_metrics.csv
â”‚   â””â”€â”€ claims_validation.csv
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ prompt_variations.csv
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ gpt4_neutral_run1.txt
â”‚   â”‚   â”œâ”€â”€ claude_positive_run1.txt
â”‚   â”‚   â””â”€â”€ gemini_negative_run1.txt
â”‚   â””â”€â”€ combined_llm_responses.csv
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ sanitize_players.py
â”‚   â”œâ”€â”€ ground_truth.py
â”‚   â”œâ”€â”€ experiment_design.py
â”‚   â”œâ”€â”€ run_experiment.py
â”‚   â””â”€â”€ validate_claims.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ hypotheses.md
â””â”€â”€ README.md

---

## ğŸ§¾ Deliverables

- âœ… Anonymized FIFA dataset (`fifa_anon.csv`)  
- âœ… Ground truth metrics  
- âœ… Structured LLM prompts  
- âœ… Raw and consolidated model outputs  
- âœ… Validation results comparing LLM claims vs numeric truth  
- âœ… `README.md` and `hypotheses.md` documentation  

---

## ğŸ§­ Learnings

This task refined the following skills:
- **Experimental design** for AI behavior analysis  
- **Data sanitization and bias detection** workflows  
- **Automation scripting** in Python (pandas, pathlib)  
- **Statistical validation and reproducibility**  
- **Ethical documentation and transparency in AI research**

---
